{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standardized measure of uncertainty\n",
    "## share of the estimate that the error represents\n",
    "## higher number means more uncertainty\n",
    "\n",
    "def coefficient_of_variation(estimate, moe):\n",
    "    \"\"\"\n",
    "    Estimate a coefficient of variation to help interpret the uncertainty of an estimate.\n",
    "    This diagnostic comes from `Splelman and Folch '15'`_ and references `American Community Survey materials`_.\n",
    "    Args:\n",
    "        estimate (float): \n",
    "        design_factor (float): \n",
    "    Returns:\n",
    "        A two-item tuple with the median followed by the approximated margin of error.\n",
    "        (42211.096153846156, 10153.200960954948)\n",
    "    Examples:\n",
    "        Estimating the median for a range of median household incomes.\n",
    "        >>> coefficient_of_variation(42211.096153846156, 10153.200960954948)\n",
    "        0.14622123567658166\n",
    "    ... _Splelman and Folch '15':\n",
    "        https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0115626#abstract0\n",
    "    ... _American Community Survey materials:\n",
    "        http://www.loc.gov/catdir/toc/ecip0720/2007024090.html\n",
    "    \"\"\"\n",
    "    numerator = moe / 1.645\n",
    "    cv = numerator / estimate\n",
    "    #if cv > 0.12:\n",
    "        #warnings.warn(\"\", CVWarning) ## “reasonable standard of precision for an estimate” exceeded\n",
    "\n",
    "    return cv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14622123567658166"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_of_variation(42211.096153846156, 10153.200960954948)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantify information loss through aggregation\n",
    "\n",
    "def information_loss(old_region_data, makeup, new_region_data):\n",
    "    \"\"\"\n",
    "    Measure whether the new estimates for a given variable are within the margins of error of their original components.\n",
    "\n",
    "    Information is considered to be lost if the 90 percent confidence interval of original geography value\n",
    "    estimates do not overlap with the new geography's estimate. This diagnostic comes from `Splelman and Folch '15'`_.\n",
    "    Args:\n",
    "        old_region_data (list of dictionaries): \n",
    "            Each dictionary should have two keys:\n",
    "                * val (float): The estimate for the old region\n",
    "                * moe (float): The margin of error on the estimate for the old region\n",
    "        makeup (list of lists): inner lists are length of new geographies, outer list is length of old geographies,\n",
    "                                the values in inner list j partition old region j into each of the new regions\n",
    "        new_region_data (list): The estimate for the new region\n",
    "    Returns:\n",
    "        A two-item tuple with an array of information loss metrics per new geographic region and an aggregated information loss across all the regions\n",
    "        ([0.0, 0.0], 0.0)\n",
    "    Examples:\n",
    "        >>> information_loss(test_input, makeup_input, disaggregate_sum(test_input, makeup_input, deterministic = True)[0])\n",
    "        ([0.0, 0.0], 0.0)\n",
    "    ... _Splelman and Folch '15':\n",
    "        https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0115626#abstract0\n",
    "    \"\"\"\n",
    "    check_by_new_region = []\n",
    "    for i in range(len(old_region_data)):\n",
    "        check_one_region = []\n",
    "        for j in range(len(makeup[i])):\n",
    "            if makeup[i][j]!=0:  # if old region has been partially partitioned into new region\n",
    "                check_one_region.append(abs(old_region_data[i][0]['val']-new_region_data[j]) < old_region_data[i][0]['moe'])  # see if new estimate lies in old confidence interval\n",
    "                # NOTE: maybe too stringent, see what the values look like for LA, should this also account for new geography's moe?\n",
    "        check_by_new_region.append(sum(check_one_region)/len(check_one_region))  # aggregate metric for all old values that go into the new one\n",
    "    return check_by_new_region, sum(check_by_new_region)/len(check_by_new_region)  # aggregate metric across all new geographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "household_income_2013_1 = [\n",
    "            \n",
    "        dict( n=900, val = 900,  moe=20) ## in special case of sum, val is the same as n, but expecting different naming\n",
    "            \n",
    "        ]\n",
    "\n",
    "household_income_la_2013_2 = [\n",
    "      \n",
    "      dict( n=1927, val = 1927, moe = 30)\n",
    "  ]\n",
    "\n",
    "## note aggregator will need to provide n and moe n as outputs somehow for this\n",
    "\n",
    "\n",
    "test_input = [household_income_2013_1,household_income_la_2013_2]\n",
    "\n",
    "makeup_1 = [0.8, 0.2] ## 80% of old geography 1 goes to new geography 1, 20% of old geography 1 goes to new geograph 2\n",
    "makeup_2 =  [0.3, 0.7] ## 30% of old geography 2 goes to new geography 1, 70% of old geography 2 goes to new geograph 2\n",
    "\n",
    "makeup_input = [makeup_1, makeup_2] ## should have a test that each sums to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as of 2019-08-26\n",
    "def disaggregate_sum(data_list, makeup, deterministic = True, simulations=50):\n",
    "    \"\"\"\n",
    "    Take aggregated sums for a set of geographies and disaggregate into new sums for a different set of geographies\n",
    "    Args:\n",
    "        data_list (list of dictionaries): \n",
    "            Each dictionary should have two keys:\n",
    "                * n (int): The number of people, households or other unit in the old region\n",
    "                * moe (float): the margin of error on the n for the old region\n",
    "        makeup (list of lists): inner lists are length of new geographies, outer list is length of old geographies,\n",
    "                                the values in inner list j partition old region j into each of the new regions\n",
    "    Returns:\n",
    "        list of two arrays (estimate of n values for new regions, margins of error for new n values)\n",
    "    Examples:\n",
    "        >>> disaggregate_sum(data_list, makeup)\n",
    "        (array([1298.1, 1528.9]), None)\n",
    "        >>> disaggregate_sum(data_list, makeup, deterministic = F)\n",
    "        (array([1298.502, 1528.938]), array([13.933, 27.603]))\n",
    "        \"\"\"\n",
    "    for m in makeup_input:\n",
    "        if not math.isclose(sum(m), 1, abs_tol = .001):\n",
    "            warnings.warn(\"\", PartitionWarning) ## at least one of your partitions does not sum to one\n",
    "\n",
    "    #  if you just want an estimate and ignore margin of error\n",
    "    if deterministic:\n",
    "        results_sum = []\n",
    "        for i in range(len(data_list)):\n",
    "            total = []\n",
    "            results_sum.append([data_list[i][0]['n']*z for z in makeup[i]]) #  get proportion of n going to each new geography\n",
    "        rr = numpy.vstack(results_sum) #  get into array format\n",
    "        est = numpy.apply_along_axis(sum, 0, rr) #  sum each column (total for each new geography)\n",
    "        return est, None\n",
    "    # otherwise deal with margin of error via simulation\n",
    "    else:\n",
    "        simulation_results = []\n",
    "        for s in range(simulations):\n",
    "            results_sum = []\n",
    "            for i in range(len(data_list)):\n",
    "                total = []\n",
    "                se = data_list[i][0]['moe'] / 1.645 #  convert moe to se\n",
    "                new_n = round(numpy.random.normal(data_list[i][0]['n'], se)) # u se moe to introduce randomness into number in bin\n",
    "                new_n = int(new_n) #  clean it up\n",
    "                results_sum.append([new_n*z for z in makeup[i]]) #  get proportion of simulated n going to each new geography\n",
    "            rr = numpy.vstack(results_sum) # get into array format\n",
    "            simulation_results.append(numpy.apply_along_axis(sum, 0, rr)) #  sum each column (total for each new geography)\n",
    "        ss = numpy.vstack(simulation_results) #  get simulation into array format\n",
    "        est = numpy.apply_along_axis(numpy.mean, 0, ss) #  mean across simulations\n",
    "        t1 = numpy.apply_along_axis(numpy.quantile, axis=0, arr =ss, q=0.95) - est #  higher quantile across simulations\n",
    "        t2 = est - numpy.apply_along_axis(numpy.quantile, axis=0, arr = ss, q=0.05) #  lower quantile across simulations\n",
    "        margin_of_error = numpy.amax(numpy.column_stack((t1,t2)), 1) #  take the larger of each row to be conservative\n",
    "        return est, margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1298.1, 1528.9]), None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test =disaggregate_sum(test_input, makeup_input, deterministic = True, simulations=50)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 0.0], 0.0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_loss(test_input, makeup_input, test[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
