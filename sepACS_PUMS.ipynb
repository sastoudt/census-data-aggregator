{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stats.stackexchange.com/questions/13169/defining-quantiles-over-a-weighted-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_median_acs(range_list, sampling_percentage=None):\n",
    "    \"\"\"\n",
    "    Estimate a median and approximate the margin of error.\n",
    "    Follows the U.S. Census Bureau's `official guidelines`_ for estimation using a design factor.\n",
    "    Useful for generating medians for measures like household income and age when aggregating census geographies.\n",
    "    Args:\n",
    "        range_list (list): A list of dictionaries that divide the full range of data values into continuous categories.\n",
    "            Each dictionary should have three keys:\n",
    "                * min (int): The minimum value of the range\n",
    "                * max (int): The maximum value of the range\n",
    "                * n (int): The number of people, households or other unit in the range\n",
    "            The minimum value in the first range and the maximum value in the last range can be tailored to the dataset\n",
    "            by using the \"jam values\" provided in the `American Community Survey's technical documentation`_.\n",
    "        sampling_percentage (float, optional): A statistical input used to correct variance for finite population. This value\n",
    "            represents the percentage of the population that was sampled to create the data. If you do not provide this input, a\n",
    "            margin of error will not be returned.\n",
    "    Returns:\n",
    "        A two-item tuple with the median followed by the approximated margin of error.\n",
    "        (42211.096153846156, 10153.200960954948)\n",
    "    Examples:\n",
    "        Estimating the median for a range of household incomes.\n",
    "        >>> household_income_2013_acs5 = [\n",
    "            dict(min=2499, max=9999, n=186),\n",
    "            dict(min=10000, max=14999, n=78),\n",
    "            dict(min=15000, max=19999, n=98),\n",
    "            dict(min=20000, max=24999, n=287),\n",
    "            dict(min=25000, max=29999, n=142),\n",
    "            dict(min=30000, max=34999, n=90),\n",
    "            dict(min=35000, max=39999, n=107),\n",
    "            dict(min=40000, max=44999, n=104),\n",
    "            dict(min=45000, max=49999, n=178),\n",
    "            dict(min=50000, max=59999, n=106),\n",
    "            dict(min=60000, max=74999, n=177),\n",
    "            dict(min=75000, max=99999, n=262),\n",
    "            dict(min=100000, max=124999, n=77),\n",
    "            dict(min=125000, max=149999, n=100),\n",
    "            dict(min=150000, max=199999, n=58),\n",
    "            dict(min=200000, max=250001, n=18)\n",
    "        ]\n",
    "        >>> approximate_median_acs(household_income_2013_acs5, sampling_percentage=5*2.5)\n",
    "        (42211.096153846156, 4706.522752733644)\n",
    "    ... _official guidelines:\n",
    "        https://www.documentcloud.org/documents/6165603-2013-2017AccuracyPUMS.html#document/p18\n",
    "    ... _American Community Survey's technical documentation\n",
    "        https://www.documentcloud.org/documents/6165752-2017-SummaryFile-Tech-Doc.html#document/p20/a508561\n",
    "    ... _the bureau's reference material:\n",
    "        https://www.census.gov/programs-surveys/acs/technical-documentation/pums/documentation.html\n",
    "    \"\"\"\n",
    "    # Sort the list\n",
    "    range_list.sort(key=lambda x: x['min'])\n",
    "\n",
    "    # For each range calculate its min and max value along the universe's scale\n",
    "    cumulative_n = 0\n",
    "    for range_ in range_list:\n",
    "        range_['n_min'] = cumulative_n\n",
    "        cumulative_n += range_['n']\n",
    "        range_['n_max'] = cumulative_n\n",
    "\n",
    "    # What is the total number of observations in the universe?\n",
    "    n = sum([d['n'] for d in range_list])\n",
    "\n",
    "    # What is the estimated midpoint of the n?\n",
    "    n_midpoint = n / 2.0\n",
    "\n",
    "    # Now use those to determine which group contains the midpoint.\n",
    "    n_midpoint_range = next(d for d in range_list if n_midpoint >= d['n_min'] and n_midpoint <= d['n_max'])\n",
    "\n",
    "    # How many households in the midrange are needed to reach the midpoint?\n",
    "    n_midrange_gap = n_midpoint - n_midpoint_range['n_min']\n",
    "\n",
    "    # What is the proportion of the group that would be needed to get the midpoint?\n",
    "    n_midrange_gap_percent = n_midrange_gap / n_midpoint_range['n']\n",
    "\n",
    "    # Apply this proportion to the width of the midrange\n",
    "    n_midrange_gap_adjusted = (n_midpoint_range['max'] - n_midpoint_range['min']) * n_midrange_gap_percent\n",
    "\n",
    "    # Estimate the median\n",
    "    estimated_median = n_midpoint_range['min'] + n_midrange_gap_adjusted\n",
    "\n",
    "    # If there's no sampling percentage, we can't calculate a margin of error\n",
    "    if not sampling_percentage:\n",
    "        # Let's throw a warning, but still return the median\n",
    "        warnings.warn(\"\", SamplingPercentageWarning)\n",
    "        return estimated_median, None\n",
    "\n",
    "    # Get the standard error for this dataset\n",
    "    standard_error = (math.sqrt(((100 - sampling_percentage) / (n * sampling_percentage)) * (50**2))) / 100\n",
    "\n",
    "    # Use the standard error to calculate the p values\n",
    "    p_lower = (.5 - standard_error)\n",
    "    p_upper = (.5 + standard_error)\n",
    "\n",
    "    # Estimate the p_lower and p_upper n values\n",
    "    p_lower_n = n * p_lower\n",
    "    p_upper_n = n * p_upper\n",
    "\n",
    "    # Find the ranges the p values fall within\n",
    "    try:\n",
    "        p_lower_range_i, p_lower_range = next(\n",
    "            (i, d) for i, d in enumerate(range_list)\n",
    "            if p_lower_n >= d['n_min'] and p_lower_n <= d['n_max']\n",
    "        )\n",
    "    except StopIteration:\n",
    "        raise DataError(f\"The n's lower p value {p_lower_n} does not fall within a data range.\")\n",
    "\n",
    "    try:\n",
    "        p_upper_range_i, p_upper_range = next(\n",
    "            (i, d) for i, d in enumerate(range_list)\n",
    "            if p_upper_n >= d['n_min'] and p_upper_n <= d['n_max']\n",
    "        )\n",
    "    except StopIteration:\n",
    "        raise DataError(f\"The n's upper p value {p_upper_n} does not fall within a data range.\")\n",
    "\n",
    "    # Use these values to estimate the lower bound of the confidence interval\n",
    "    p_lower_a1 = p_lower_range['min']\n",
    "    try:\n",
    "        p_lower_a2 = range_list[p_lower_range_i + 1]['min']\n",
    "    except IndexError:\n",
    "        p_lower_a2 = p_lower_range['max']\n",
    "    p_lower_c1 = p_lower_range['n_min'] / n\n",
    "    try:\n",
    "        p_lower_c2 = range_list[p_lower_range_i + 1]['n_min'] / n\n",
    "    except IndexError:\n",
    "        p_lower_c2 = p_lower_range['n_max'] / n\n",
    "    lower_bound = ((p_lower - p_lower_c1) / (p_lower_c2 - p_lower_c1)) * (p_lower_a2 - p_lower_a1) + p_lower_a1\n",
    "\n",
    "    # Same for the upper bound\n",
    "    p_upper_a1 = p_upper_range['min']\n",
    "    try:\n",
    "        p_upper_a2 = range_list[p_upper_range_i + 1]['min']\n",
    "    except IndexError:\n",
    "        p_upper_a2 = p_upper_range['max']\n",
    "    p_upper_c1 = p_upper_range['n_min'] / n\n",
    "    try:\n",
    "        p_upper_c2 = range_list[p_upper_range_i + 1]['n_min'] / n\n",
    "    except IndexError:\n",
    "        p_upper_c2 = p_upper_range['n_max'] / n\n",
    "    upper_bound = ((p_upper - p_upper_c1) / (p_upper_c2 - p_upper_c1)) * (p_upper_a2 - p_upper_a1) + p_upper_a1\n",
    "\n",
    "    # Calculate the standard error of the median\n",
    "    standard_error_median = 0.5 * (upper_bound - lower_bound)\n",
    "\n",
    "    # Calculate the margin of error at the 90% confidence level\n",
    "    margin_of_error = 1.645 * standard_error_median\n",
    "\n",
    "    # Return the result\n",
    "    return estimated_median, margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_median_pums(range_list, sampling_percentage=None, design_factor=None):\n",
    "    \"\"\"\n",
    "    Estimate a weighted median and approximate the margin of error.\n",
    "    Args:\n",
    "        range_list (list): A list of dictionaries that divide the full range of data values into continuous categories.\n",
    "            Each dictionary should have three keys:\n",
    "                * val (float): value for individual\n",
    "                * w (int): weight for the individual\n",
    "        design_factor (float, optional): A statistical input used to tailor the standard error to the\n",
    "            variance of the dataset. This is only needed for data coming from PUMS. The Census Bureau publishes design factors as\n",
    "            part of its PUMS Accuracy statement.\n",
    "            Find the value for the dataset you are estimating by referring to `the bureau's reference material`_.\n",
    "            If you do not provide this input, the default is one which will have no effect on the margin of error.\n",
    "        sampling_percentage (float, optional): A statistical input used to correct variance for finite population. This value\n",
    "            represents the percentage of the population that was sampled to create the data. If you do not provide this input, a\n",
    "            margin of error will not be returned.\n",
    "    Returns:\n",
    "        A two-item tuple with the weighted median followed by the approximated margin of error.\n",
    "        (42211.096153846156, 10153.200960954948)\n",
    "    Examples:\n",
    "        Estimating the median for a range of household incomes.\n",
    "        >>> dummy_PUMS_data = [\n",
    "            dict(val=5000,  w=186/2068),\n",
    "            dict(val=12000,  w=78/2068),\n",
    "            dict(val=17000, w=98/2068),\n",
    "            dict(val=23000,  w=287/2068),\n",
    "            dict(val=27000,  w=142/2068),\n",
    "            dict(val=31000,  w=90/2068),\n",
    "            dict(val=36000,  w=107/2068),\n",
    "            dict(val=42000,  w=104/2068),\n",
    "            dict(val=48000,  w=178/2068),\n",
    "            dict(val=59000,  w=106/2068),\n",
    "            dict(val=63000,  w=177/2068),\n",
    "            dict(val=90000,  w=262/2068),\n",
    "            dict(val=110000,  w=77/2068),\n",
    "            dict(val=135000,  w=100/2068),\n",
    "            dict(val=180000,  w=58/2068),\n",
    "            dict(val=210000,  w=18/2068)\n",
    "]\n",
    "        >>> approximate_median_pums(dummy_PUMS_data, design_factor=1, sampling_percentage=1)\n",
    "        (41865.16853932583, 20257.481963744707)\n",
    "        \"\"\"\n",
    "\n",
    "    est = weighted_quantile(range_list, 0.5)\n",
    "    \n",
    "    # If there's no sampling percentage, we can't calculate a margin of error\n",
    "    if not sampling_percentage:\n",
    "        # Let's throw a warning, but still return the median\n",
    "        warnings.warn(\"\", SamplingPercentageWarning)\n",
    "        return estimated_median, None\n",
    "\n",
    "    \n",
    "    # What is the total number of observations in the universe?\n",
    "    n = len(range_list)\n",
    "    \n",
    "    # Get the standard error for this dataset\n",
    "    standard_error = (design_factor * math.sqrt(((100 - sampling_percentage) / (n * sampling_percentage)) * (50**2))) / 100\n",
    "    \n",
    "    #https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/quantse.htm\n",
    "    est75 = weighted_quantile(range_list, 0.75)\n",
    "    est25 = weighted_quantile(range_list, 0.25)\n",
    "    \n",
    "    h = (1.2 * (est75-est25))/(n ** (1/5))\n",
    "\n",
    "    \n",
    "    val = []\n",
    "    for range_ in range_list:\n",
    "        val.append(range_['val'])\n",
    "    \n",
    "    val=numpy.array(val)\n",
    "    \n",
    "    # nint = how many data points fall in est +/- h\n",
    "    k = numpy.where(( val<=est+h) & ( val>=est-h)) \n",
    "\n",
    "    nint = len(k[0])\n",
    "    \n",
    "    fhat= nint/(2*n*h)\n",
    "   \n",
    "    standard_error_median = 1/(2*math.sqrt(n)*fhat)\n",
    "    \n",
    "    # Calculate the margin of error at the 90% confidence level\n",
    "    margin_of_error = 1.645 * standard_error_median\n",
    "\n",
    "    # Return the result\n",
    "    return est, margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exceptions.py\n",
    "class DesignFactorWarning(Warning):\n",
    "    \"\"\"\n",
    "    Warns that you have not provided a design factor.\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"\"\"A margin of error cannot be calculated unless you provide a design factor.\n",
    "Design factors for different census surveys and tables can be found in the \"PUMS Accuracy\" CSV files. https://www.census.gov/programs-surveys/acs/technical-documentation/pums/documentation.html\n",
    "\"\"\"\n",
    "\n",
    "## init\n",
    "from .exceptions import DesignFactorWarning\n",
    "\n",
    "## test.py\n",
    "\n",
    "def test_exception(self):\n",
    "        DesignFactorWarning().__str__()\n",
    "        \n",
    "with self.assertWarns(DesignFactorWarning):\n",
    "            m, moe = census_data_aggregator.approximate_median(income)\n",
    "            self.assertTrue(moe == None)      \n",
    "            \n",
    "import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_weighted_quantile(range_list, k):\n",
    "    \n",
    "    n = len(range_list)\n",
    "   \n",
    "    S_k = (k-1) * range_list[k-1]['w'] + (n - 1) * range_list[k-2]['w_max']\n",
    " \n",
    "    return S_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_quantile(data_list, p):\n",
    "    \n",
    "#https://stats.stackexchange.com/questions/13169/defining-quantiles-over-a-weighted-sample\n",
    "    \n",
    "    data_list.sort(key=lambda x: x['val'])\n",
    "    \n",
    "    cumulative_w = 0\n",
    "    for range_ in data_list:\n",
    "        range_['w_min'] = cumulative_w\n",
    "        cumulative_w += range_['w']\n",
    "        range_['w_max'] = cumulative_w \n",
    "    \n",
    "    n=len(data_list)\n",
    "    S_n = (n-1) * cumulative_w\n",
    "\n",
    "    S_k = []\n",
    "    S_k1 = []\n",
    "    for i in range(n-1)[2:n-1]:\n",
    "        S_k.append(helper_weighted_quantile(data_list, i))\n",
    "        S_k1.append(helper_weighted_quantile(data_list,i+1))\n",
    "    \n",
    "    interp = numpy.array(numpy.divide(S_k, S_n))\n",
    "    interp2 = numpy.array(numpy.divide(S_k1, S_n))\n",
    "\n",
    "    k = numpy.where((interp<=p) & (p<=interp2))[0] \n",
    "    ## if 0 it's really 2 which is really 1 :/\n",
    "\n",
    "    ## check these indices;\n",
    "    k=k[0]\n",
    "    est = data_list[k+1]['val']+(data_list[k+2]['val'] - data_list[k+1]['val'])*(p*S_n-S_k[k])/(S_k1[k]-S_k[k])\n",
    "    return est\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_income_2013_acs5 = [\n",
    "            dict(val=5000,  w=186/2068),\n",
    "            dict(val=12000,  w=78/2068),\n",
    "            dict(val=17000, w=98/2068),\n",
    "            dict(val=23000,  w=287/2068),\n",
    "            dict(val=27000,  w=142/2068),\n",
    "            dict(val=31000,  w=90/2068),\n",
    "            dict(val=36000,  w=107/2068),\n",
    "            dict(val=42000,  w=104/2068),\n",
    "            dict(val=48000,  w=178/2068),\n",
    "            dict(val=59000,  w=106/2068),\n",
    "            dict(val=63000,  w=177/2068),\n",
    "            dict(val=90000,  w=262/2068),\n",
    "            dict(val=110000,  w=77/2068),\n",
    "            dict(val=135000,  w=100/2068),\n",
    "            dict(val=180000,  w=58/2068),\n",
    "            dict(val=210000,  w=18/2068)\n",
    "]\n",
    "\n",
    "weighted_quantile(household_income_2013_acs5,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_weighted_quantile(household_income_2013_acs5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41865.16853932583, 20257.481963744707)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approximate_median_pums(household_income_2013_acs5, sampling_percentage=1, design_factor=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems reasonable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
