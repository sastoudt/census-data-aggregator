{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disaggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs: each old geography has an estimate and a margin of error (output from census aggregator), then it has the proportion of the old geography that goes in each of the new geographies\n",
    "\n",
    "\n",
    "Outputs: estimates and margins of error for the new geographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "household_income_2013_1 = [\n",
    "            \n",
    "        dict( n=900, moe=8)\n",
    "            \n",
    "        ]\n",
    "\n",
    "household_income_la_2013_2 = [\n",
    "      \n",
    "      dict( n=1927, moe = 50)\n",
    "  ]\n",
    "\n",
    "## note aggregator will need to provide n and moe n as outputs somehow for this\n",
    "\n",
    "\n",
    "test_input = [household_income_2013_1,household_income_la_2013_2]\n",
    "\n",
    "makeup_1 = [0.8, 0.2] ## 80% of old geography 1 goes to new geography 1, 20% of old geography 1 goes to new geograph 2\n",
    "makeup_2 =  [0.3, 0.7]\n",
    "\n",
    "makeup_input = [makeup_1, makeup_2] ## should have a test that each sums to 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disaggregate_sum(data_list, makeup, deterministic = True, simulations=50):\n",
    "    \"\"\"\n",
    "    Take aggregated sums for a set of geographies and disaggregate into new sums for a different set of geographies\n",
    "    Args:\n",
    "        data_list (list of dictionaries): \n",
    "            Each dictionary should have two keys:\n",
    "                * n (int): The number of people, households or other unit in the old region\n",
    "                * moe (float): the margin of error on the n for the old region\n",
    "        makeup (list of lists): inner list is length of new geographies, outer list is length of old geographies,\n",
    "                                the values in inner list j partition old region j into each of the new regions\n",
    "    Returns:\n",
    "        list of two arrays (estimate of n values for new regions, margins of error for new n values)\n",
    "    Examples:\n",
    "        >>> disaggregate_sum(data_list, makeup)\n",
    "        (array([1298.1, 1528.9]), None)\n",
    "        >>> disaggregate_sum(data_list, makeup, deterministic = F)\n",
    "        (array([1298.502, 1528.938]), array([13.933, 27.603]))\n",
    "        \"\"\"\n",
    "    \n",
    "    #  if you just want an estimate and ignore margin of error\n",
    "    if deterministic:\n",
    "        results_sum = []\n",
    "        for i in range(len(data_list)):\n",
    "            total = []\n",
    "            results_sum.append([data_list[i][0]['n']*z for z in makeup[i]]) #  get proportion of n going to each new geography\n",
    "        rr = numpy.vstack(results_sum) #  get into array format\n",
    "        est = numpy.apply_along_axis(sum, 0, rr) #  sum each column (total for each new geography)\n",
    "        return est, None\n",
    "    # otherwise deal with margin of error via simulation\n",
    "    else:\n",
    "        simulation_results = []\n",
    "        for s in range(simulations):\n",
    "            results_sum = []\n",
    "            for i in range(len(data_list)):\n",
    "                total = []\n",
    "                se = data_list[i][0]['moe'] / 1.645 #  convert moe to se\n",
    "                new_n = round(numpy.random.normal(data_list[i][0]['n'], se)) # u se moe to introduce randomness into number in bin\n",
    "                new_n = int(new_n) #  clean it up\n",
    "                results_sum.append([new_n*z for z in makeup[i]]) #  get proportion of simulated n going to each new geography\n",
    "            rr = numpy.vstack(results_sum) # get into array format\n",
    "            simulation_results.append(numpy.apply_along_axis(sum, 0, rr)) #  sum each column (total for each new geography)\n",
    "        ss = numpy.vstack(simulation_results) #  get simulation into array format\n",
    "        est = numpy.apply_along_axis(numpy.mean, 0, ss) #  mean across simulations\n",
    "        t1 = numpy.apply_along_axis(numpy.quantile, axis=0, arr =ss, q=0.95) - est #  higher quantile across simulations\n",
    "        t2 = est - numpy.apply_along_axis(numpy.quantile, axis=0, arr = ss, q=0.05) #  lower quantile across simulations\n",
    "        margin_of_error = numpy.amax(numpy.column_stack((t1,t2)), 1) #  take the larger of each row to be conservative\n",
    "        return est, margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1298.1, 1528.9]), None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaggregate_sum(test_input, makeup_input, deterministic = True, simulations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1298.502, 1528.938]), array([13.933, 27.603]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaggregate_sum(test_input, makeup_input, deterministic = False, simulations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disaggregate_mean(data_list, makeup, deterministic = True, simulations=50):\n",
    "    \"\"\"\n",
    "    Take aggregated means for a set of geographies and disaggregate into new means for a different set of geographies\n",
    "    Args:\n",
    "        data_list (list of dictionaries): \n",
    "            Each dictionary should have two keys:\n",
    "                * n (int): The number of people, households or other unit in the old region\n",
    "                * val (float): The estimate in the old region\n",
    "                * moe_n (float): The moe for the n\n",
    "                * moe_val (float): The moe for the value\n",
    "        makeup (list of lists): inner list is length of new geographies, outer list is length of old geographies,\n",
    "                                the values in inner list j partition old region j into each of the new regions\n",
    "    Returns:\n",
    "        list of two arrays (estimate of mean values for new regions, margins of error for new mean values)\n",
    "    Examples:\n",
    "        >>> disaggregate_mean(data_list, makeup)\n",
    "        (array([37573.13793103, 44330.25      , 49929.        ]), None)\n",
    "        >>> disaggregate_mean(data_list, makeup, deterministic = F)\n",
    "        (array([37571.20936   , 44317.40688447, 49928.91120178]),\n",
    "         array([ 33.46206937, 127.01398643,   9.02957785]))\n",
    "        \"\"\"\n",
    "    \n",
    "    for m in makeup:\n",
    "        if not math.isclose(sum(m), 1, abs_tol=10**-3): ## to three decimal places\n",
    "            warnings.warn(\"\", PartitionWarning) ## at least one of your partitions does not sum to one\n",
    "    \n",
    "    #  if you just want an estimate and ignore margin of error\n",
    "    if deterministic:\n",
    "        denom = disaggregate_sum(data_list, makeup, deterministic = True, simulations=50)[0] #  get the weighted sum for denominator of weighted mean\n",
    "        results = []\n",
    "        grand_total = []\n",
    "        for i in range(len(data_list)):\n",
    "            total = data_list[i][0]['n'] * data_list[i][0]['val'] #  weighted value\n",
    "            results.append([total*z for z in makeup[i]])  #  partition by new geograph\n",
    "        rr = numpy.vstack(results) #  get in array format\n",
    "        rr2 = numpy.apply_along_axis(sum, 0, rr) #  weighted sum\n",
    "        est = numpy.divide(rr2, denom) #  get the weighted average\n",
    "        return est, None\n",
    "    #  otherwise deal with margin of error via simulation\n",
    "    else:\n",
    "        simulation_results = [[] for i in range(simulations)]\n",
    "        #simulation_results\n",
    "        for s in range(simulations):\n",
    "            results=[[] for i in range(len(makeup[0]))] ## number of new polygons is how many the old polygons are split into\n",
    "            for i in range(len(data_list)):\n",
    "                se_n = data_list[i][0]['moe_n'] / 1.645 #  convert moe to se\n",
    "                se_val=data_list[i][0]['moe_val'] / 1.645 #  convert moe to se\n",
    "                new_n = round(numpy.random.normal(data_list[i][0]['n'], se_n))  \n",
    "                #  use moe to introduce randomness into number in bin\n",
    "                new_n = int(new_n) #  clean it up\n",
    "                for j in range(len(makeup[0])):\n",
    "                    new_n_piece = round(new_n * makeup[i][j])\n",
    "                    #  use moe to introduce randomness into value\n",
    "                    #  draw individuals based on this new estimate, partitioned into new geographies\n",
    "                    results[j].append(numpy.random.normal(data_list[i][0]['val'], se_val, new_n_piece)) \n",
    "            mean_results = [] ## for new geographies\n",
    "            for j in range(len(makeup[0])):\n",
    "                mean_results.append(numpy.mean(numpy.concatenate(results[j])))\n",
    "            simulation_results[s]=mean_results\n",
    "        ss = numpy.vstack(simulation_results) #  get simulation into array format\n",
    "        est = numpy.apply_along_axis(numpy.nanmean, 0, ss) #  take mean of means across simulations\n",
    "        t1 = numpy.apply_along_axis(numpy.nanquantile, axis=0, arr =ss, q=0.95) - est #  upper quantile of means across simulations\n",
    "        t2 = est - numpy.apply_along_axis(numpy.nanquantile, axis=0, arr = ss, q=0.05) #  lower quantile of means across simulations\n",
    "        margin_of_error = numpy.amax(numpy.column_stack((t1,t2)), 1) #  take max across rows to be conservative\n",
    "        return est, margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_income_2013_acs5 = [\n",
    "            \n",
    "            dict( n=90, moe_n=1,val=34999, moe_val=25)\n",
    "            \n",
    "        ]\n",
    "\n",
    "household_income_la_2013_acs1 = [\n",
    "      \n",
    "      dict( n=50, moe_n = 1, val=49929, moe_val=25)\n",
    "  ]\n",
    "\n",
    "\n",
    "test_input = [household_income_2013_acs5,household_income_la_2013_acs1]\n",
    "\n",
    "makeup_1 = [0.8, 0.2,0]\n",
    "makeup_2 =  [0.3, 0.6,0.1]\n",
    "#makeup_3 = [0.5, 0.4, 0.1]\n",
    "\n",
    "## test that these proportions times n round to at least one\n",
    "## else warning, moe might be deceptively small\n",
    "\n",
    "makeup_input = [makeup_1, makeup_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([37573.13793103, 44330.25      , 49929.        ]), None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaggregate_mean(test_input, makeup_input, deterministic = True, simulations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([37571.20936   , 44317.40688447, 49928.91120178]),\n",
       " array([ 33.46206937, 127.01398643,   9.02957785]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=disaggregate_mean(test_input, makeup_input, deterministic = False, simulations=50)\n",
    "#len(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## median\n",
    "\n",
    "def disaggregate_median(data_list, makeup, deterministic = True, simulations=50):\n",
    "    \"\"\"\n",
    "    Take aggregated medians for a set of geographies and disaggregate into new medians for a different set of geographies\n",
    "    Args:\n",
    "        data_list (list of dictionaries): \n",
    "            Each dictionary should have two keys:\n",
    "                * n (int): The number of people, households or other unit in the old region\n",
    "                * val (float): The estimate in the old region\n",
    "                * moe_n (float): The moe for the n\n",
    "                * moe_val (float): The moe for the value\n",
    "        makeup (list of lists): inner list is length of new geographies, outer list is length of old geographies,\n",
    "                                the values in inner list j partition old region j into each of the new regions\n",
    "    Returns:\n",
    "        list of two arrays (estimate of median values for new regions, margins of error for new median values)\n",
    "    Examples:\n",
    "        >>> disaggregate_median(data_list, makeup)\n",
    "        ([34999.0, 49929.0, 49929.0], None)\n",
    "        >>> disaggregate_median(data_list, makeup, deterministic = F)\n",
    "        (array([35003.25758268, 49917.76306533, 49925.44251865]),\n",
    "            array([2.16161205, 3.19795328, 9.34154849]))\n",
    "        \"\"\"\n",
    "    for m in makeup:\n",
    "        if not math.isclose(sum(m), 1, abs_tol=10**-3): ## to three decimal places\n",
    "            warnings.warn(\"\", PartitionWarning) ## at least one of your partitions does not sum to one\n",
    "            \n",
    "    #  if you just want an estimate and ignore margin of error\n",
    "    if deterministic:\n",
    "        new_medians = []\n",
    "        for j in range(len(makeup[0])):  \n",
    "            results = []\n",
    "            for i in range(len(data_list)):\n",
    "                results.append(numpy.repeat(data_list[i][0]['val'], round(numpy.prod([data_list[i][0]['n'],makeup[i][j]])), axis=0))\n",
    "            new_medians.append(numpy.median(numpy.concatenate(results)))\n",
    "        return new_medians, None\n",
    "    #  otherwise deal with margin of error via simulation\n",
    "    else:\n",
    "        simulation_results = [[] for i in range(simulations)]\n",
    "        #simulation_results\n",
    "        for s in range(simulations):\n",
    "            results=[[] for i in range(len(makeup[0]))] ## number of new polygons is how many the old polygons are split into\n",
    "            for i in range(len(data_list)):\n",
    "                se_n = data_list[i][0]['moe_n'] / 1.645 #  convert moe to se\n",
    "                se_val=data_list[i][0]['moe_val'] / 1.645 #  convert moe to se\n",
    "                new_n = round(numpy.random.normal(data_list[i][0]['n'], se_n))  \n",
    "                #  use moe to introduce randomness into number in bin\n",
    "                new_n = int(new_n) #  clean it up\n",
    "                for j in range(len(makeup[0])):\n",
    "                    new_n_piece = round(new_n * makeup[i][j])\n",
    "                    #  use moe to introduce randomness into value\n",
    "                    #  draw individuals based on this new estimate, partitioned into new geographies\n",
    "                    results[j].append(numpy.random.normal(data_list[i][0]['val'], se_val, new_n_piece)) \n",
    "            median_results = [] ## for new geographies\n",
    "            for j in range(len(makeup[0])):\n",
    "                median_results.append(numpy.median(numpy.concatenate(results[j])))\n",
    "            simulation_results[s]=median_results\n",
    "        ss = numpy.vstack(simulation_results) #  get simulation into array format\n",
    "        est = numpy.apply_along_axis(numpy.nanmean, 0, ss) #  take mean of medians across simulations\n",
    "        t1 = numpy.apply_along_axis(numpy.nanquantile, axis=0, arr =ss, q=0.95) - est #  upper quantile of medians across simulations\n",
    "        t2 = est - numpy.apply_along_axis(numpy.nanquantile, axis=0, arr = ss, q=0.05) #  lower quantile of medians across simulations\n",
    "        margin_of_error = numpy.amax(numpy.column_stack((t1,t2)), 1) #  take max across rows to be conservative\n",
    "        return est, margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([34999.0, 49929.0, 49929.0], None)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaggregate_median(test_input, makeup_input, deterministic = True, simulations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([35003.25758268, 49917.76306533, 49925.44251865]),\n",
       " array([2.16161205, 3.19795328, 9.34154849]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaggregate_median(test_input, makeup_input, deterministic = False, simulations=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warnings to include somewhere in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in makeup_input:\n",
    "    if not math.isclose(sum(m), 1, abs_tol = .001):\n",
    "        warnings.warn(\"\", PartitionWarning) ## at least one of your partitions does not sum to one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test that these proportions times n round to at least one\n",
    "## else warning, moe might be deceptively small\n",
    "\n",
    "## min in makeup times that is not zero? all n values it will interact with\n",
    "## but could have others contributing that will make it fine?\n",
    "\n",
    "## total in a bucket can't be too close to zero\n",
    "\n",
    "## \n",
    "\n",
    "## also when drawing from normal could be negative if mean is too small \n",
    "numpy.concatenate(makeup_input)\n",
    "\n",
    "for m in makeup_input:\n",
    "    if not math.isclose(sum(m), 1, abs_tol = .001):\n",
    "        warnings.warn(\"\", SmallPopulationWarning) ## moe might be deceptively small if a region has too few people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 0.2, 0. , 0.3, 0.6, 0.1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
